{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "a365ComputeOptions": null,
    "sessionKeepAliveTimeout": 0,
    "trident": {
      "lakehouse": {
        "default_lakehouse": "{SILVER_LAKEHOUSE_ID}",
        "default_lakehouse_name": "{SILVER_LAKEHOUSE__NAME}",
        "default_lakehouse_workspace_id": "{WORKSPACE_ID}",
        "known_lakehouses": [
          { "id": "{SILVER_LAKEHOUSE_ID}" },
          { "id": "{BRONZE_LAKEHOUSE_ID}" }
        ]
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "protocol = \"abfss://\"\n",
        "onelake_path = \"onelake.dfs.fabric.microsoft.com/\"\n",
        "workspace_id = \"{WORKSPACE_ID}\"\n",
        "lakehouse_id = \"{BRONZE_LAKEHOUSE_ID}\"\n",
        "lakehouse_path = protocol  + workspace_id + \"@\" + onelake_path + \"/\" + lakehouse_id"
      ],
      "outputs": [
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "cellStatus": "{\"Ted Pattison\":{\"queued_time\":\"2024-01-26T15:30:51.3532625Z\",\"session_start_time\":null,\"execution_start_time\":\"2024-01-26T15:30:51.810043Z\",\"execution_finish_time\":\"2024-01-26T15:30:52.6985612Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}"
      },
      "id": "16ab6c17-ceb8-4c1c-b3a7-32851d5f9412"
    },
    {
      "cell_type": "code",
      "source": [
        "# create products table for silver layer\n",
        "from pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType\n",
        "\n",
        "# create schema for products table using StructType and StructField \n",
        "schema_products = StructType([\n",
        "    StructField(\"ProductId\", LongType() ),\n",
        "    StructField(\"Product\", StringType() ),\n",
        "    StructField(\"Category\", StringType() )\n",
        "])\n",
        "\n",
        "# Load CSV file into Spark DataFrame and validate data using schema\n",
        "df_products = (\n",
        "    spark.read.format(\"csv\")\n",
        "         .option(\"header\",\"true\")\n",
        "         .schema(schema_products)\n",
        "         .load(lakehouse_path + \"/Files/Products.csv\")\n",
        ")\n",
        "\n",
        "# save DataFrame as lakehouse table in Delta format\n",
        "( df_products.write\n",
        "             .mode(\"overwrite\")\n",
        "             .option(\"overwriteSchema\", \"True\")\n",
        "             .format(\"delta\")\n",
        "             .save(\"Tables/products\")\n",
        ")"
      ],
      "id": "87c4e2f5-628b-4d65-85c4-3e715605a246"
    },
    {
      "cell_type": "code",
      "source": [
        "# create customers table for silver layer\n",
        "from pyspark.sql.types import StructType, StructField, StringType, LongType, DateType\n",
        "\n",
        "# create schema for customers table using StructType and StructField \n",
        "schema_customers = StructType([\n",
        "    StructField(\"CustomerId\", LongType() ),\n",
        "    StructField(\"FirstName\", StringType() ),\n",
        "    StructField(\"LastName\", StringType() ),\n",
        "    StructField(\"Country\", StringType() ),\n",
        "    StructField(\"City\", StringType() ),\n",
        "    StructField(\"DOB\", DateType() ),\n",
        "])\n",
        "\n",
        "# Load CSV file into Spark DataFrame with schema and support to infer dates\n",
        "df_customers = (\n",
        "    spark.read.format(\"csv\")\n",
        "         .option(\"header\",\"true\")\n",
        "         .schema(schema_customers)\n",
        "         .option(\"dateFormat\", \"MM/dd/yyyy\")\n",
        "         .option(\"inferSchema\", \"true\")\n",
        "         .load(lakehouse_path + \"/Files/Customers.csv\")\n",
        ")\n",
        "\n",
        "# save DataFrame as lakehouse table in Delta format\n",
        "( df_customers.write\n",
        "              .mode(\"overwrite\")\n",
        "              .option(\"overwriteSchema\", \"True\")\n",
        "              .format(\"delta\")\n",
        "              .save(\"Tables/customers\")\n",
        ")"
      ],
     
      "id": "f2fed1a2-1674-4846-85cb-adfe31fc6f82"
    },
    {
      "cell_type": "code",
      "source": [
        "# create invoices table for silver layer\n",
        "from pyspark.sql.types import StructType, StructField, LongType, FloatType, DateType\n",
        "\n",
        "# create schema for invoices table using StructType and StructField \n",
        "schema_invoices = StructType([\n",
        "    StructField(\"InvoiceId\", LongType() ),\n",
        "    StructField(\"Date\", DateType() ),\n",
        "    StructField(\"TotalSalesAmount\", FloatType() ),\n",
        "    StructField(\"CustomerId\", LongType() )\n",
        "])\n",
        "\n",
        "# Load CSV file into Spark DataFrame with schema and support to infer dates\n",
        "df_invoices = (\n",
        "    spark.read.format(\"csv\")\n",
        "         .option(\"header\",\"true\")\n",
        "         .schema(schema_invoices)\n",
        "         .option(\"dateFormat\", \"MM/dd/yyyy\")\n",
        "         .option(\"inferSchema\", \"true\") \n",
        "         .load(lakehouse_path + \"/Files/Invoices.csv\")\n",
        ")\n",
        "\n",
        "# save DataFrame as lakehouse table in Delta format\n",
        "( df_invoices.write\n",
        "             .mode(\"overwrite\")\n",
        "             .option(\"overwriteSchema\", \"True\")\n",
        "             .format(\"delta\")\n",
        "             .save(\"Tables/invoices\")\n",
        ")"
      ],      
      "id": "3d4b151e-f206-4bde-b498-61e7c63d224c"
    },
    {
      "cell_type": "code",
      "source": [
        "# create invoice_details table for silver layer\n",
        "from pyspark.sql.types import StructType, StructField, LongType, FloatType\n",
        "\n",
        "# create schema for invoice_details table using StructType and StructField \n",
        "schema_invoice_details = StructType([\n",
        "    StructField(\"Id\", LongType() ),\n",
        "    StructField(\"Quantity\", LongType() ),\n",
        "    StructField(\"SalesAmount\", FloatType() ),\n",
        "    StructField(\"InvoiceId\", LongType() ),\n",
        "    StructField(\"ProductId\", LongType() )\n",
        "])\n",
        "\n",
        "# Load CSV file into Spark DataFrame and validate data using schema\n",
        "df_invoice_details = (\n",
        "    spark.read.format(\"csv\")\n",
        "         .option(\"header\",\"true\")\n",
        "         .schema(schema_invoice_details)\n",
        "         .load(lakehouse_path + \"/Files/InvoiceDetails.csv\")\n",
        ")\n",
        "\n",
        "# save DataFrame as lakehouse table in Delta format\n",
        "( df_invoice_details.write\n",
        "                    .mode(\"overwrite\")\n",
        "                    .option(\"overwriteSchema\", \"True\")\n",
        "                    .format(\"delta\")\n",
        "                    .save(\"Tables/invoice_details\")\n",
        ")"
      ],     
      "id": "6bfc808f-7531-4896-bd42-e0fa8cf19467"
    }
  ]
}