{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "a365ComputeOptions": null,
    "sessionKeepAliveTimeout": 0,
    "trident": {
      "lakehouse": {
        "default_lakehouse": "{GOLD_LAKEHOUSE_ID}",
        "default_lakehouse_name": "{GOLD_LAKEHOUSE__NAME}",
        "default_lakehouse_workspace_id": "{WORKSPACE_ID}",
        "known_lakehouses": [
          { "id": "{GOLD_LAKEHOUSE_ID}" },
          { "id": "{SILVER_LAKEHOUSE_ID}" }
        ]
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "protocol = \"abfss://\"\n",
        "onelake_path = \"onelake.dfs.fabric.microsoft.com/\"\n",
        "workspace_id = \"{WORKSPACE_ID}\"\n",
        "lakehouse_id = \"{SILVER_LAKEHOUSE_ID}\"\n",
        "lakehouse_path = protocol  + workspace_id + \"@\" + onelake_path + \"/\" + lakehouse_id"
      ]

    },
    {
      "cell_type": "code",
      "source": [
        "# create products table for gold layer\n",
        "\n",
        "# load DataFrame from silver layer table\n",
        "df_gold_products = (\n",
        "    spark.read\n",
        "         .format(\"delta\")\n",
        "         .load(lakehouse_path + \"/Tables/products\")\n",
        ")\n",
        "\n",
        "# write DataFrame to new gold layer table \n",
        "( df_gold_products.write\n",
        "                  .mode(\"overwrite\")\n",
        "                  .option(\"overwriteSchema\", \"True\")\n",
        "                  .format(\"delta\")\n",
        "                  .save(\"Tables/products\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create customers table for gold layer\n",
        "from pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\n",
        "\n",
        "# load DataFrame from silver layer table and perform transforms\n",
        "df_gold_customers = (\n",
        "    spark.read\n",
        "         .format(\"delta\")\n",
        "         .load(lakehouse_path + \"/Tables/customers\")\n",
        "         .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\n",
        "         .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \n",
        "         .drop('FirstName', 'LastName')\n",
        ")\n",
        "\n",
        "# write DataFrame to new gold layer table \n",
        "( df_gold_customers.write\n",
        "                   .mode(\"overwrite\")\n",
        "                   .option(\"overwriteSchema\", \"True\")\n",
        "                   .format(\"delta\")\n",
        "                   .save(\"Tables/customers\")\n",
        ")"
      ]

    },
    {
      "cell_type": "code",
      "source": [
        "# create sales table for gold layer\n",
        "from pyspark.sql.functions import col, desc, concat, lit, floor, datediff\n",
        "from pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\n",
        "\n",
        "# load DataFrames using invoices table and invoice_details table from silver layer\n",
        "df_silver_invoices = spark.read.format(\"delta\").load(lakehouse_path + \"/Tables/invoices\")\n",
        "df_silver_invoice_details = spark.read.format(\"delta\").load(lakehouse_path + \"/Tables/invoice_details\")\n",
        "\n",
        "# perform join to merge columns from both DataFrames and transform data \n",
        "df_gold_sales = (\n",
        "    df_silver_invoice_details\n",
        "        .join(df_silver_invoices, \n",
        "              df_silver_invoice_details['InvoiceId'] == df_silver_invoices['InvoiceId'])\n",
        "        .withColumnRenamed('SalesAmount', 'Sales')\n",
        "        .withColumn(\"DateKey\", (year(col('Date'))*10000) + \n",
        "                               (month(col('Date'))*100) + \n",
        "                               (dayofmonth(col('Date')))   )\n",
        "        .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\n",
        "        .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\n",
        ")\n",
        "\n",
        "# write DataFrame to new gold layer table \n",
        "( df_gold_sales.write\n",
        "               .mode(\"overwrite\")\n",
        "               .option(\"overwriteSchema\", \"True\")\n",
        "               .format(\"delta\")\n",
        "               .save(\"Tables/sales\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create calendar table for gold layer\n",
        "from datetime import date\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek, date_format\n",
        "\n",
        "# get first and last calendar date from sakes table \n",
        "first_sales_date = df_gold_sales.agg({\"Date\": \"min\"}).collect()[0][0]\n",
        "last_sales_date = df_gold_sales.agg({\"Date\": \"max\"}).collect()[0][0]\n",
        "\n",
        "# calculate start date and end date for calendar table\n",
        "start_date = date(first_sales_date.year, 1, 1)\n",
        "end_date = date(last_sales_date.year, 12, 31)\n",
        "\n",
        "# create pandas DataFrame with Date series column\n",
        "df_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\n",
        "\n",
        "# convert pandas DataFrame to Spark DataFrame and add calculated calendar columns\n",
        "df_calendar_spark = (\n",
        "     spark.createDataFrame(df_calendar_ps)\n",
        "       .withColumnRenamed(\"0\", \"timestamp\")\n",
        "       .withColumn(\"Date\", to_date(col('timestamp')))\n",
        "       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \n",
        "                              (month(col('timestamp'))*100) + \n",
        "                              (dayofmonth(col('timestamp')))   )\n",
        "       .withColumn(\"Year\", year(col('timestamp'))  )\n",
        "       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-QQ\")  )\n",
        "       .withColumn(\"Month\", date_format(col('timestamp'),'yyyy-MM')  )\n",
        "       .withColumn(\"Day\", dayofmonth(col('timestamp'))  )\n",
        "       .withColumn(\"MonthInYear\", date_format(col('timestamp'),'MMMM')  )\n",
        "       .withColumn(\"MonthInYearSort\", month(col('timestamp'))  )\n",
        "       .withColumn(\"DayOfWeek\", date_format(col('timestamp'),'EEEE')  )\n",
        "       .withColumn(\"DayOfWeekSort\", dayofweek(col('timestamp')))\n",
        "       .drop('timestamp')\n",
        ")\n",
        "\n",
        "# write DataFrame to new gold layer table \n",
        "( df_calendar_spark.write\n",
        "                   .mode(\"overwrite\")\n",
        "                   .option(\"overwriteSchema\", \"True\")\n",
        "                   .format(\"delta\")\n",
        "                   .save(\"Tables/calendar\")\n",
        ")\n"
      ]
    }
  ]
}