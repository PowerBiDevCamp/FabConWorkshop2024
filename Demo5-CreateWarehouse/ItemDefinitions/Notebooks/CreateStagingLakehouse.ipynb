{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "a365ComputeOptions": null,
    "sessionKeepAliveTimeout": 0,
    "trident": {
      "lakehouse": {
        "default_lakehouse": "{LAKEHOUSE_ID}",
        "default_lakehouse_name": "{LAKEHOUSE_NAME}",
        "default_lakehouse_workspace_id": "{WORKSPACE_ID}",
        "known_lakehouses": [
          {
            "id": "{LAKEHOUSE_ID}"
          }
        ]
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
      },
      "source": [
        "# copy CSV files to lakehouse to load data into bronze layer \n",
        "import requests\n",
        "\n",
        "csv_base_url = \"https://github.com/PowerBiDevCamp/ProductSalesData/raw/main/\"\n",
        "\n",
        "csv_files = { \"Customers.csv\", \"Products.csv\", \"Invoices.csv\", \"InvoiceDetails.csv\" }\n",
        "\n",
        "folder_path = \"Files/bronze_landing_layer/\"\n",
        "\n",
        "for csv_file in csv_files:\n",
        "    csv_file_path = csv_base_url + csv_file\n",
        "    with requests.get(csv_file_path) as response:\n",
        "        csv_content = response.content.decode('utf-8-sig')\n",
        "        mssparkutils.fs.put(folder_path + csv_file, csv_content, True)\n",
        "        print(csv_file + \" copied to Lakehouse file in OneLake\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
      },
      "source": [
        "# create products table for silver layer\n",
        "from pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType\n",
        "\n",
        "# create schema for products table using StructType and StructField \n",
        "schema_products = StructType([\n",
        "    StructField(\"ProductId\", LongType() ),\n",
        "    StructField(\"Product\", StringType() ),\n",
        "    StructField(\"Category\", StringType() )\n",
        "])\n",
        "\n",
        "# Load CSV file into Spark DataFrame and validate data using schema\n",
        "df_products = (\n",
        "    spark.read.format(\"csv\")\n",
        "         .option(\"header\",\"true\")\n",
        "         .schema(schema_products)\n",
        "         .load(\"Files/bronze_landing_layer/Products.csv\")\n",
        ")\n",
        "\n",
        "# save DataFrame as lakehouse table in Delta format\n",
        "( df_products.write\n",
        "             .mode(\"overwrite\")\n",
        "             .option(\"overwriteSchema\", \"True\")\n",
        "             .format(\"delta\")\n",
        "             .save(\"Tables/products\")\n",
        ")\n",
        "\n",
        "# display table schema and data\n",
        "df_products.printSchema()\n",
        "df_products.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
      },
      "source": [
        "# create customers table for silver layer\n",
        "from pyspark.sql.types import StructType, StructField, StringType, LongType, DateType\n",
        "\n",
        "# create schema for customers table using StructType and StructField \n",
        "schema_customers = StructType([\n",
        "    StructField(\"CustomerId\", LongType() ),\n",
        "    StructField(\"FirstName\", StringType() ),\n",
        "    StructField(\"LastName\", StringType() ),\n",
        "    StructField(\"Country\", StringType() ),\n",
        "    StructField(\"City\", StringType() ),\n",
        "    StructField(\"DOB\", DateType() ),\n",
        "])\n",
        "\n",
        "# Load CSV file into Spark DataFrame with schema and support to infer dates\n",
        "df_customers = (\n",
        "    spark.read.format(\"csv\")\n",
        "         .option(\"header\",\"true\")\n",
        "         .schema(schema_customers)\n",
        "         .option(\"dateFormat\", \"MM/dd/yyyy\")\n",
        "         .option(\"inferSchema\", \"true\")\n",
        "         .load(\"Files/bronze_landing_layer/Customers.csv\")\n",
        ")\n",
        "\n",
        "# save DataFrame as lakehouse table in Delta format\n",
        "( df_customers.write\n",
        "              .mode(\"overwrite\")\n",
        "              .option(\"overwriteSchema\", \"True\")\n",
        "              .format(\"delta\")\n",
        "              .save(\"Tables/customers\")\n",
        ")\n",
        "\n",
        "# display table schema and data\n",
        "df_customers.printSchema()\n",
        "df_customers.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
      },
      "source": [
        "# create invoices table for silver layer\n",
        "from pyspark.sql.types import StructType, StructField, LongType, FloatType, DateType\n",
        "\n",
        "# create schema for invoices table using StructType and StructField \n",
        "schema_invoices = StructType([\n",
        "    StructField(\"InvoiceId\", LongType() ),\n",
        "    StructField(\"Date\", DateType() ),\n",
        "    StructField(\"TotalSalesAmount\", FloatType() ),\n",
        "    StructField(\"CustomerId\", LongType() )\n",
        "])\n",
        "\n",
        "# Load CSV file into Spark DataFrame with schema and support to infer dates\n",
        "df_invoices = (\n",
        "    spark.read.format(\"csv\")\n",
        "         .option(\"header\",\"true\")\n",
        "         .schema(schema_invoices)\n",
        "         .option(\"dateFormat\", \"MM/dd/yyyy\")\n",
        "         .option(\"inferSchema\", \"true\") \n",
        "         .load(\"Files/bronze_landing_layer/Invoices.csv\")\n",
        ")\n",
        "\n",
        "# save DataFrame as lakehouse table in Delta format\n",
        "( df_invoices.write\n",
        "             .mode(\"overwrite\")\n",
        "             .option(\"overwriteSchema\", \"True\")\n",
        "             .format(\"delta\")\n",
        "             .save(\"Tables/invoices\")\n",
        ")\n",
        "\n",
        "# display table schema and data\n",
        "df_invoices.printSchema()\n",
        "df_invoices.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
      },
      "source": [
        "# create invoice_details table for silver layer\n",
        "from pyspark.sql.types import StructType, StructField, LongType, FloatType\n",
        "\n",
        "# create schema for invoice_details table using StructType and StructField \n",
        "schema_invoice_details = StructType([\n",
        "    StructField(\"Id\", LongType() ),\n",
        "    StructField(\"Quantity\", LongType() ),\n",
        "    StructField(\"SalesAmount\", FloatType() ),\n",
        "    StructField(\"InvoiceId\", LongType() ),\n",
        "    StructField(\"ProductId\", LongType() )\n",
        "])\n",
        "\n",
        "# Load CSV file into Spark DataFrame and validate data using schema\n",
        "df_invoice_details = (\n",
        "    spark.read.format(\"csv\")\n",
        "         .option(\"header\",\"true\")\n",
        "         .schema(schema_invoice_details)\n",
        "         .load(\"Files/bronze_landing_layer/InvoiceDetails.csv\")\n",
        ")\n",
        "\n",
        "# save DataFrame as lakehouse table in Delta format\n",
        "( df_invoice_details.write\n",
        "                    .mode(\"overwrite\")\n",
        "                    .option(\"overwriteSchema\", \"True\")\n",
        "                    .format(\"delta\")\n",
        "                    .save(\"Tables/invoice_details\")\n",
        ")\n",
        "\n",
        "# display table schema and data\n",
        "df_invoice_details.printSchema()\n",
        "df_invoice_details.show()"
      ]
    }
  ]
}